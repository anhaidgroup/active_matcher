{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e727bb9",
   "metadata": {},
   "source": [
    "# ActiveMatcher Tutorial\n",
    "\n",
    "This tutorial describes how to do entity matching using Active Matcher. Specifically, we will see how to \n",
    "train a model using active learning and then apply the model. We do this with the following steps,\n",
    "\n",
    "0. Setup\n",
    "1. Reading in Data\n",
    "2. Creating a Model\n",
    "3. Selecting Features\n",
    "4. Generating Feature Vectors\n",
    "5. Selecting Seeds\n",
    "6. Training the Model with Active Learning \n",
    "7. Applying the Trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b1d3d",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before we begin, we first need to import all of the necessary packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a321d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from active_matcher.active_learning import EntropyActiveLearner\n",
    "from active_matcher.fv_generator import FVGenerator\n",
    "from active_matcher.feature_selector import FeatureSelector\n",
    "from active_matcher.ml_model import  SKLearnModel, SparkMLModel\n",
    "from active_matcher.labeler import  GoldLabeler\n",
    "from active_matcher.algorithms import select_seeds\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "from pathlib import Path\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849da5f",
   "metadata": {},
   "source": [
    "Next we need to initialize Spark, for this example we are just going to run in local mode, however ActiveMatcher can also run on a cluster seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0a7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 15:02:11 WARN Utils: Your hostname, Devs-MacBook-Pro-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.70 instead (on interface en0)\n",
      "25/04/20 15:02:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 15:02:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark =  SparkSession.builder\\\n",
    "                        .master('local[*]')\\\n",
    "                        .config('spark.sql.execution.arrow.pyspark.enabled',  'true')\\\n",
    "                        .getOrCreate()\n",
    "\n",
    "#shutil.make_archive('active_matcher', 'zip', '../')\n",
    "#spark.sparkContext.addPyFile('active_matcher.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f5966",
   "metadata": {},
   "source": [
    "## 1. Reading in Data\n",
    "\n",
    "Once we have the SparkSession initialized, we can read in the raw data along with our candidate set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c979dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data_dir = Path('./data/dblp_acm/')\n",
    "A = spark.read.parquet(str(data_dir / 'table_a.parquet'))\n",
    "B = spark.read.parquet(str(data_dir / 'table_b.parquet'))\n",
    "cand = spark.read.parquet(str(data_dir / 'cand.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48b6cb",
   "metadata": {},
   "source": [
    "Both A and B can are just typical relational tables, in this example each row in the table refers to a paper citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5a2b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-----------------+----+\n",
      "|_id|               title|             authors|            venue|year|\n",
      "+---+--------------------+--------------------+-----------------+----+\n",
      "|  0|semantic integrat...|     d. scott mackay|    sigmod record|1999|\n",
      "|  1|estimation of que...|viswanath poosala...|             vldb|1996|\n",
      "|  2|incremental maint...|themistoklis palp...|             vldb|2002|\n",
      "|  3|cost-based select...|zhao-hui tang , g...|             vldb|1996|\n",
      "|  4|benchmarking spat...|erik g. hoel , ha...|             vldb|1995|\n",
      "|  5|efficient geometr...|      daniel a. keim|sigmod conference|1999|\n",
      "|  6|mining the world ...|      aris m. ouksel|    sigmod record|2002|\n",
      "|  7|enhanced abstract...|    praveen seshadri|          vldb j.|1998|\n",
      "|  8|report on dart ' ...|nandit soparkar ,...|    sigmod record|1997|\n",
      "|  9|unisql 's next-ge...|phil janus , albe...|    sigmod record|1996|\n",
      "| 10|dual-buffering st...|alfons kemper , d...|             vldb|1994|\n",
      "| 11|aurora : a data s...|nesime tatbul , d...|sigmod conference|2003|\n",
      "| 12|priority assignme...|rajendran m. siva...|          vldb j.|1996|\n",
      "| 13|wavecluster : a w...|surojit chatterje...|          vldb j.|2000|\n",
      "| 14|approximate query...|hector garcia-mol...|          vldb j.|2001|\n",
      "| 15|sprint : a scalab...|john c. shafer , ...|             vldb|1996|\n",
      "| 16|snowball : a prot...|jeff pavel , luis...|sigmod conference|2001|\n",
      "| 17|a framework for i...|timothy griffin ,...|sigmod conference|1997|\n",
      "| 18|parametric query ...|raymond t. ng , t...|          vldb j.|1997|\n",
      "| 19|lineage tracing f...|yingwei cui , jen...|             vldb|2001|\n",
      "+---+--------------------+--------------------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f033f",
   "metadata": {},
   "source": [
    "Our candidate set is a set of rolled up pairs, where `cand['id2']` refers to the `B['_id']` of the record in table B and the ids in `cand['id1_list']` refer to the records in table A with ids `A['_id']`. We use this format for improving effeciency of generating feature vectors, especially when `cand` is produced by a top-k blocking algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00e428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "|id2|            id1_list|            is_train|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[398, 525, 420, 7...|[true, true, true...|\n",
      "|  1|[314, 2576, 1979,...|[true, true, true...|\n",
      "|  2|[2395, 953, 635, ...|[true, true, true...|\n",
      "|  3|          [463, 548]|        [true, true]|\n",
      "|  4|[2131, 973, 69, 7...|[true, true, true...|\n",
      "|  5|              [1146]|              [true]|\n",
      "|  6|  [1698, 2324, 1528]|  [true, true, true]|\n",
      "|  7|[2407, 1290, 1671...|[true, true, fals...|\n",
      "|  8|         [1194, 177]|       [true, false]|\n",
      "|  9|[2303, 2485, 1945...|[true, true, true...|\n",
      "| 10|[1244, 2005, 662,...|[true, true, fals...|\n",
      "| 11|              [1258]|              [true]|\n",
      "| 12|           [675, 31]|       [true, false]|\n",
      "| 13|         [2464, 607]|        [true, true]|\n",
      "| 14|         [789, 2086]|       [false, true]|\n",
      "| 15|               [622]|              [true]|\n",
      "| 16|         [1667, 509]|       [false, true]|\n",
      "| 17|[1307, 1225, 890,...|[true, true, true...|\n",
      "| 18|    [1657, 1481, 30]|  [true, true, true]|\n",
      "| 19|         [695, 2017]|       [true, false]|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cand.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737b401",
   "metadata": {},
   "source": [
    "Next we can create a labeler, for this example, we use gold data to create an automatic labeler, however the `Labeler` class can be subclassed to add a human in the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3ce6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_parquet(data_dir / 'gold.parquet')\n",
    "gold = set(zip(gold_df.id1, gold_df.id2))\n",
    "labeler = GoldLabeler(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be0bb2",
   "metadata": {},
   "source": [
    "## 2. Creating a Model\n",
    "\n",
    "Next we can choose a model to train. In this example we are using `XGBClassifier`. Notice that we pass the type of model, not a model instance. Additionally, we can pass model specific keyword args as we would when constructing the model normally, in this case we passed, \n",
    "```python\n",
    "eval_metric='logloss', objective='binary:logistic', max_depth=6, seed=42\n",
    "```\n",
    "Note that while we use `XGBClassifier` in this example, most any model that\n",
    "exposes a scikit-learn interface should work with two important caveats.\n",
    "\n",
    "#### Model Training and Inference Time\n",
    "First, for each iteration in active learning, requries training a new model and then applying the model to each feature vector we are doing active learning on. This means that if model training and/or inference are slow, the active learning process will be very slow. \n",
    "\n",
    "#### Model Threading\n",
    "Second, many algorithms use multiple threads for training and inference. Since training takes place on the spark driver node, it is okay if model training with multiple threads. For inference the model *should not* use multiple threads as it will cause significant over subscription of the processor and lead to extremely slow model inference times (including during active learning). Fortunately, sklearn provides an easy way to disable threading \n",
    "using `threadpoolctl`, `SKLearnModel` automatically disables threading for inference using `threadpoolctl` meaning that sklearn models shouldn't require any modification and can be passed to `SKLearnModel` unchanged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ac99f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SKLearnModel(XGBClassifier, eval_metric='logloss', objective='binary:logistic', max_depth=6, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0775e",
   "metadata": {},
   "source": [
    "## 3. Selecting Features\n",
    "\n",
    "With all of that set up, we can now select features that we will use to generate feature vectors for each pair in `cand`. Here we use the default typical set of features, however `extra_features` can be set to `True` which will cause the code to generate _significantly_ more features, and likely improve model accuracy at the cost of increased runtime for feature vector generation and active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5368aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[feature_selector.py:114 - select_features() ] 2025-04-20 15:02:20,000 : 0 columns dropped because more than 50.0% of values were null or nan\n",
      "[feature_selector.py:115 - select_features() ] 2025-04-20 15:02:20,003 : columns after dropping nulls :\n",
      " ['title', 'authors', 'venue', 'year']\n",
      "[feature_selector.py:118 - select_features() ] 2025-04-20 15:02:20,004 : numeric columns in dataframe :\n",
      " ['year']\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "selector = FeatureSelector(extra_features=False)\n",
    "\n",
    "features = selector.select_features(A.drop('_id'), B.drop('_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd43ba",
   "metadata": {},
   "source": [
    "## 4. Generating Feature Vectors\n",
    "\n",
    "Now that we have selected features, we can generate feature vectors for each pair in `cand`. First we need to build the features and then we can generate the actual feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b8db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[fv_generator.py:182 - build() ] 2025-04-20 15:02:29,265 : building features\n",
      "[fv_generator.py:158 - _create_sqlite_df() ] 2025-04-20 15:04:04,074 : preprocesing data\n",
      "[fv_generator.py:158 - _create_sqlite_df() ] 2025-04-20 15:04:04,079 : preprocesing data\n",
      "[fv_generator.py:164 - _create_sqlite_df() ] 2025-04-20 15:04:04,811 : constructing sqlite df\n",
      "[fv_generator.py:164 - _create_sqlite_df() ] 2025-04-20 15:04:04,811 : constructing sqlite df\n",
      "[fv_generator.py:205 - generate_fvs() ] 2025-04-20 15:04:28,287 : generating features\n",
      "[fv_generator.py:106 - _gen_fvs() ] 2025-04-20 15:04:28,584 : schema of fvs StructType([StructField('id2', LongType(), True), StructField('is_train', BooleanType(), True), StructField('id1', LongType(), True), StructField('fv', ArrayType(FloatType(), True), True)])\n"
     ]
    }
   ],
   "source": [
    "fv_gen = FVGenerator(features)\n",
    "fv_gen.build(A, B)\n",
    "fvs = fv_gen.generate_fvs(cand)\n",
    "fvs = model.prep_fvs(fvs, 'features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d90a9",
   "metadata": {},
   "source": [
    "## 5. Selecting Seeds\n",
    "\n",
    "Once we have the feature vectors, we can select seeds for active learning, for this operation we need to score each pair which is _positively_ correlated with being a match. That is the higher the score for the pair the more likely it is to be a match. In this example, we just take the sum of all the components of the feature vector for each pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d18d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "fvs = fvs.withColumn('score', F.aggregate('features', F.lit(0.0), lambda acc, x : acc + F.when(x.isNotNull() & ~F.isnan(x), x).otherwise(0.0) ))\n",
    "seeds = select_seeds(fvs, 'score', 50, labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbaacd",
   "metadata": {},
   "source": [
    "## 6. Training the Model with Active Learning\n",
    "\n",
    "Next we run active learning, for at most 50 iterations with a batch size of 10. This process will then output a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188985bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ent_active_learner.py:128 - train() ] 2025-04-20 15:05:12,572 : max iter = 50  \n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:12,579 : starting iteration 0\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:12,581 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:13,117 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:19,982 : new batch positive = 9.0 negative = 1.0, total positive = 32.0 negative = 28.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:19,986 : starting iteration 1\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:19,986 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:20,710 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:28,622 : new batch positive = 7.0 negative = 3.0, total positive = 39.0 negative = 31.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:28,637 : starting iteration 2\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:28,648 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:29,774 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:34,570 : new batch positive = 10.0 negative = 0.0, total positive = 49.0 negative = 31.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:34,572 : starting iteration 3\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:34,572 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:34,954 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:39,971 : new batch positive = 7.0 negative = 3.0, total positive = 56.0 negative = 34.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:39,972 : starting iteration 4\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:39,972 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:40,269 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:46,052 : new batch positive = 10.0 negative = 0.0, total positive = 66.0 negative = 34.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:46,057 : starting iteration 5\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:46,057 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:46,359 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:51,130 : new batch positive = 5.0 negative = 5.0, total positive = 71.0 negative = 39.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:51,137 : starting iteration 6\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:51,138 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:51,514 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:05:55,859 : new batch positive = 10.0 negative = 0.0, total positive = 81.0 negative = 39.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:05:55,860 : starting iteration 7\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:05:55,860 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:05:56,210 : selecting and labeling new examples\n",
      "[ent_active_learner.py:159 - train() ] 2025-04-20 15:06:01,121 : new batch positive = 0.0 negative = 10.0, total positive = 81.0 negative = 49.0\n",
      "[ent_active_learner.py:130 - train() ] 2025-04-20 15:06:01,122 : starting iteration 8\n",
      "[ent_active_learner.py:132 - train() ] 2025-04-20 15:06:01,122 : training model\n",
      "[ent_active_learner.py:140 - train() ] 2025-04-20 15:06:01,409 : selecting and labeling new examples\n"
     ]
    }
   ],
   "source": [
    "active_learner = EntropyActiveLearner(model, labeler, batch_size=10, max_iter=50)\n",
    "trained_model = active_learner.train(fvs, seeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cf995",
   "metadata": {},
   "source": [
    "## 7. Applying the Trained Model\n",
    "We can then apply the trained model to the feature vectors, outputting the binary prediction into a `fvs['prediction']` and the confidence of the prediction to `fvs['condifidence']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae67abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs = trained_model.predict(fvs, 'features', 'prediction')\n",
    "fvs = trained_model.prediction_conf(fvs, 'features', 'confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9823d2",
   "metadata": {},
   "source": [
    "Finally, we can compute precision, recall, and f1 of the predictions made by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fvs.toPandas()\n",
    "\n",
    "predicted_matches = set(res.loc[res['prediction'].eq(1.0)][['id1', 'id2']].itertuples(name=None, index=False))\n",
    "\n",
    "true_positives = len(gold & predicted_matches)\n",
    "precision = true_positives / len(predicted_matches)\n",
    "recall = true_positives / len(gold)\n",
    "f1 = (precision * recall * 2) / (precision + recall)\n",
    "\n",
    "print(\n",
    "f'''\n",
    "{true_positives=}\n",
    "{precision=}\n",
    "{recall=}\n",
    "{f1=}\n",
    "'''\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

