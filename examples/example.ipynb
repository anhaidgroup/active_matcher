{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from active_matcher.active_learning import EntropyActiveLearner\n",
    "from active_matcher.fv_generator import FVGenerator\n",
    "from active_matcher.feature_selector import FeatureSelector\n",
    "from active_matcher.ml_model import  SKLearnModel, SparkMLModel\n",
    "from active_matcher.labeler import  GoldLabeler\n",
    "from active_matcher.algorithms import select_seeds\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849da5f",
   "metadata": {},
   "source": [
    "The first thing we need to do is initialize spark, for this example we are just going to run in local mode, however ActiveMatcher can also run on a cluster seemlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =  SparkSession.builder\\\n",
    "                        .setMaster('local[*]')\\\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f5966",
   "metadata": {},
   "source": [
    "Once we have the SparkSession initialized, we can read in the raw data along with our candidate set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = spark.read.parquet('./examples/data/dblp_acm/table_a.parquet')\n",
    "B = spark.read.parquet('./examples/data/dblp_acm/table_b.parquet')\n",
    "cand = spark.read.parquet('./examples/data/dblp_acm/cand.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48b6cb",
   "metadata": {},
   "source": [
    "Both A and B can are just tpyical relationale tables, in this example each row in the table refers to a paper citation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f033f",
   "metadata": {},
   "source": [
    "Our candidate set is a set of rolled up pairs, where `cand['id2']` refers to the `B['_id']` of the record in table B and the ids in `cand['id1_list']` refer to the records in table A with ids `A['_id']`. We use this format for improving effeciency of generating feature vectors, especially when `cand` is produced by a top-k blocking algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be0bb2",
   "metadata": {},
   "source": [
    "Next we can choose a model to train. In this example we are using `XGBClassifier`. Notice that we pass the type of model, not a model instance. Additionally, we can pass model specific keyword args as we would when constructing the model normally, in this case we passed, \n",
    "```python\n",
    "eval_metric='logloss', objective='binary:logistic', max_depth=6, seed=42\n",
    "```\n",
    "Note that while we use `XGBClassifier` in this example, most any model that\n",
    "exposes a scikit-learn interface should work with two important caveats.\n",
    "\n",
    "### 1. Model Training and Inference Time\n",
    "First, for each iteration in active learning, requries training a new model and then applying the model to each feature vector we are doing active learning on. This means that if model training and/or inference are slow, the active learning process will be very slow. \n",
    "\n",
    "### 2. Model Threading\n",
    "Second, many algorithms use multiple threads for training and inference. Since training takes place on the spark driver node, it is okay if model training with multiple threads. For inference the model *should not* use multiple threads as it will cause significant over subscription of the processor and lead to extremely slow model inference times (including during active learning). Fortunately, sklearn provides an easy way to disable threading \n",
    "using `threadpoolctl`, `SKLearnModel` automatically disables threading for inference using `threadpoolctl` meaning that sklearn models shouldn't require any modification and can be passed to `SKLearnModel` unchanged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac99f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SKLearnModel(XGBClassifier, eval_metric='logloss', objective='binary:logistic', max_depth=6, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737b401",
   "metadata": {},
   "source": [
    "Next we can create a labeler, for this example, we use gold data to create an automatic labeler, however the `Labeler` class can be subclassed to add a human in the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ce6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = pd.read_parquet('./examples/data/dblp_acm/gold.parquet')\n",
    "gold = set(zip(gold_df.id1, gold_df.id2))\n",
    "labeler = GoldLabeler(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b0775e",
   "metadata": {},
   "source": [
    "With all of that set up, we can now select features that we will use to generate feature vectors for each pair in `cand`. Here we use the default typical set of features, however `extra_features` can be set to `True` which will cause the code to generate _significantly_ more features, and likely improve model accuracy at the cost of increased runtime for feature vector generation and active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = FeatureSelector(extra_features=False)\n",
    "\n",
    "features = selector.select_features(A.drop('_id'), B.drop('_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd43ba",
   "metadata": {},
   "source": [
    "Now that we have selected features, we can generate feature vectors for each pair in `cand`. First we need to build the features and then we can generate the actual feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_gen = FVGenerator(features)\n",
    "fv_gen.build(A, B)\n",
    "fvs = fv_gen.generate_fvs(cands_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d90a9",
   "metadata": {},
   "source": [
    "Once we have the feature vectors, we can select seeds for active learning, for this operation we need to score each pair which is _positively_ correlated with being a match. That is the higher the score for the pair the more likely it is to be a match. In this example, we just take the sum of all the components of the feature vector for each pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs = fvs.withColumn('score', F.aggregate('features', F.lit(0.0), lambda acc, x : acc + F.when(x.isNotNull() & ~F.isnan(x), x).otherwise(0.0) ))\n",
    "seeds = select_seeds(fvs, 'score', 50, labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afbaacd",
   "metadata": {},
   "source": [
    "Next we run active learning, for at most 50 iterations with a batch size of 10. This process will then output a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188985bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "activer_learner = EntropyActiveLearner(model, labeler, batch_size=10, max_iter=50)\n",
    "\n",
    "trained_model = active_learner.train(train_fvs, seeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cf995",
   "metadata": {},
   "source": [
    "We can then apply the trained model to the feature vectors, outputting the binary prediction into a `fvs['prediction']` and the confidence of the prediction to `fvs['condifidence']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae67abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fvs = trained_model.predict(fvs, 'features', 'prediction')\n",
    "fvs = trained_model.prediction_conf(fvs, 'features', 'confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9823d2",
   "metadata": {},
   "source": [
    "Finally, we can compute precision, recall, and f1 of the predictions made by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fvs.toPandas()\n",
    "\n",
    "predicted_matches = set(res.loc[res['prediction'].eq(1.0)][['id1', 'id2']].itertuples(name=None, index=False))\n",
    "\n",
    "true_positives = len(gold & predicted_matches)\n",
    "precision = true_positives / len(predicted_matches)\n",
    "recall = true_positives / len(gold)\n",
    "f1 = (precision * recall * 2) / (precision + recall)\n",
    "\n",
    "print(\n",
    "f'''\n",
    "{true_positives=}\n",
    "{precision=}\n",
    "{recall=}\n",
    "{f1=}\n",
    "'''\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
