"""Tests for active_matcher.active_learning.ent_active_learner module.

This module implements EntropyActiveLearner for batch selection.
"""
from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from active_matcher.active_learning.ent_active_learner import EntropyActiveLearner
from active_matcher.labeler import CustomLabeler, GoldLabeler


class StopLabeler(CustomLabeler):
    def label_pair(self, row1, row2):
        if row1["_id"] == 10 and row2["_id"] == 20:
            return -1.0
        return 1.0 if (row1["_id"] + row2["_id"]) % 2 == 0 else 0.0


class TestEntropyActiveLearner:
    """Tests for EntropyActiveLearner behavior."""

    def test_init(self, spark_session, default_model):
        """Validate __init__ parameter checks and defaults."""
        labeler = GoldLabeler({(1, 2)})

        learner = EntropyActiveLearner(default_model, labeler, batch_size=5, max_iter=2, parquet_file_path="file.parquet")
        assert learner._batch_size == 5
        assert learner._max_iter == 2

        with pytest.raises(ValueError):
            EntropyActiveLearner(default_model, labeler, batch_size=0)
        with pytest.raises(ValueError):
            EntropyActiveLearner(default_model, labeler, max_iter=0)
        with pytest.raises(ValueError):
            EntropyActiveLearner(default_model, labeler, parquet_file_path="  ")

    def test_train_basic(self, spark_session, temp_dir: Path, default_model, seed_df, fvs_df):
        """Ensure train executes active learning and persists data."""
        labeler = GoldLabeler({(10, 20), (12, 22)})
        parquet_path = temp_dir / "ent_training.parquet"

        learner = EntropyActiveLearner(default_model, labeler, batch_size=1, max_iter=3, parquet_file_path=str(parquet_path))

        filtered = learner._select_training_vectors(fvs_df, [0])
        assert filtered.count() == 1
        pos, neg = learner._get_pos_negative(seed_df)
        assert pos == 1.0
        assert neg == 1.0

        trained = learner.train(fvs_df, seed_df)
        assert trained is not None
        assert trained._trained_model is not None
        assert parquet_path.exists()
        parquet_df = pd.read_parquet(parquet_path)
        assert set(["_id", "id1", "id2", "features", "label"]).issubset(parquet_df.columns)
        assert parquet_df["label"].isin([0.0, 1.0]).all()

        assert learner.local_training_fvs_ is not None
        assert "labeled_in_iteration" in learner.local_training_fvs_.columns

    def test_train_existing_data(self, spark_session, temp_dir: Path, default_model, seed_df, fvs_df):
        """Ensure existing parquet data path is used."""
        labeler = GoldLabeler({(10, 20), (12, 22)})
        parquet_path = temp_dir / "ent_training.parquet"

        learner_existing = EntropyActiveLearner(
            default_model, labeler, batch_size=1, max_iter=3, parquet_file_path=str(parquet_path)
        )
        learner_existing.train(fvs_df, seed_df)

        trained_existing = learner_existing.train(fvs_df, seed_df)
        assert trained_existing is not None
        assert trained_existing._trained_model is not None
        assert learner_existing.local_training_fvs_ is not None
        assert "labeled_in_iteration" in learner_existing.local_training_fvs_.columns

    def test_train_label_everything(self, spark_session, temp_dir: Path, default_model, seed_df):
        """Ensure label-everything path is exercised."""
        labeler = GoldLabeler({(10, 20), (12, 22)})
        fvs_with_li = [
            {"_id": 0, "id1": 10, "id2": 20, "features": [0.1, 0.2], "labeled_in_iteration": 0},
            {"_id": 1, "id1": 11, "id2": 21, "features": [0.2, 0.1], "labeled_in_iteration": 0},
        ]
        learner_label_all = EntropyActiveLearner(
            default_model, labeler, batch_size=10, max_iter=1, parquet_file_path=str(temp_dir / "ent_label_all.parquet")
        )
        learner_label_all._terminate_if_label_everything = True
        fvs_label_all = spark_session.createDataFrame(fvs_with_li)
        label_all = learner_label_all.train(fvs_label_all, seed_df)
        assert label_all is not None

    def test_train_error_pos_neg(self, spark_session, temp_dir: Path, default_model, fvs_df):
        """Ensure runtime error is raised when all labels are same."""
        labeler = GoldLabeler({(10, 20), (12, 22)})
        seed_all_positive = pd.DataFrame(
            {
                "_id": [0, 1],
                "id1": [10, 11],
                "id2": [20, 21],
                "features": [[0.1, 0.2], [0.2, 0.1]],
                "label": [1.0, 1.0],
            }
        )
        learner_error = EntropyActiveLearner(
            default_model, labeler, batch_size=1, max_iter=1, parquet_file_path=str(temp_dir / "ent_error.parquet")
        )
        with pytest.raises(RuntimeError):
            learner_error.train(fvs_df, seed_all_positive)

    def test_train_user_stop(self, spark_session, temp_dir: Path, default_model, seed_df, fvs_df, id_df_factory):
        """Ensure user stop is handled gracefully."""
        a_df = id_df_factory([10, 11, 12])
        b_df = id_df_factory([20, 21, 22])
        stop_labeler = StopLabeler(a_df, b_df)
        learner_stop = EntropyActiveLearner(
            default_model, stop_labeler, batch_size=1, max_iter=1, parquet_file_path=str(temp_dir / "ent_stop.parquet")
        )
        stopped = learner_stop.train(fvs_df, seed_df)
        assert stopped is not None
